# **Spam Detective: Machine Learning & Flask Integration Documentation**

## **1. Overview**

Spam Detective is a Flask-based web application that utilizes **Machine Learning (ML) models** to classify messages as **Spam or Ham** (non-spam). The application incorporates **Natural Language Processing (NLP)** techniques and integrates the trained ML model with a user-friendly interface for real-time spam detection.

---

## **2. Machine Learning Algorithm Used**

### **Algorithm: NaÃ¯ve Bayes Classifier (MultinomialNB)**

#### **Why NaÃ¯ve Bayes?**

- **Text Classification Efficiency**: Well-suited for spam detection due to its probabilistic approach.
- **Fast and Lightweight**: Works efficiently even with a large dataset.
- **Performs Well in NLP Tasks**: Particularly effective when features are word frequencies (Bag-of-Words model).

#### **Steps in ML Model Development**

1. **Data Collection**: We used a spam dataset containing SMS messages labeled as spam or ham.
2. **Text Preprocessing**:
   - Tokenization
   - Stopword Removal
   - Lowercasing
   - Removing Punctuation
3. **Feature Extraction**:
   - **TF-IDF Vectorization** (Term Frequency-Inverse Document Frequency)
   - Converts text messages into numerical feature vectors.
4. **Model Training**:
   - Splitting data into training (80%) and testing (20%).
   - Training a **Multinomial NaÃ¯ve Bayes Model** on the TF-IDF transformed data.
5. **Model Evaluation**:
   - **Accuracy**: \~98.9%
   - **Precision, Recall, and F1 Score Calculation**
6. **Model Serialization**:
   - The trained model and vectorizer were **saved using \*\*\*\*\*\*\*\*\*\*\*\*****`pickle`**.

**Key Libraries Used:**

```python
import pandas as pd
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import pickle
```

---

## **3. Flask Integration**

### **How ML Model is Integrated with Flask**

#### **Step 1: Loading the Pre-Trained Model in Flask**

```python
with open('model/spam_model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

with open('model/vectorizer.pkl', 'rb') as vectorizer_file:
    vectorizer = pickle.load(vectorizer_file)
```

#### **Step 2: Setting Up Flask Routes**

```python
from flask import Flask, render_template, request
app = Flask(__name__)
```

#### **Step 3: Text Preprocessing Function in Flask**

```python
import string
from nltk.corpus import stopwords

def clean_text(text):
    text = "".join([char for char in text if char not in string.punctuation])
    text = text.lower()
    text = " ".join([word for word in text.split() if word not in stopwords.words('english')])
    return text
```

#### **Step 4: Flask Route for Prediction**

```python
@app.route('/detect', methods=['POST'])
def detect():
    user_input = request.form['message']
    cleaned_input = clean_text(user_input)
    input_vector = vectorizer.transform([cleaned_input])
    prediction = model.predict(input_vector)[0]
    result = "SPAM" if prediction == 1 else "HAM"
    return render_template('result.html', result=result)
```

---

## **4. Required Packages and Installation**

Ensure the following dependencies are installed before running the Flask application.

```bash
pip install flask pandas numpy nltk sklearn gunicorn pickle5
```

### **Dependencies Used**

| Package      | Purpose                                         |
| ------------ | ----------------------------------------------- |
| Flask        | Web framework for creating the UI/API           |
| Scikit-learn | Machine learning algorithms and text processing |
| Pandas       | Data handling for text data                     |
| NLTK         | NLP functions for stopword removal              |
| Gunicorn     | Production server for deploying the app         |
| Pickle       | Model serialization and deserialization         |

---

## **5. Deployment on Render/Heroku**

1. **Prepare the Application**:

   - Ensure all files are structured properly:
     ```
     /model
        - spam_model.pkl
        - vectorizer.pkl
     /templates
        - index.html
        - detect.html
        - result.html
     /static
        - styles.css
     app.py
     requirements.txt
     Procfile (for Heroku)
     ```

2. **Create a ****************`requirements.txt`**************** File**

   ```bash
   pip freeze > requirements.txt
   ```

3. **Deploy on Render (Recommended)**

   - Push the code to **GitHub**
   - Connect GitHub to **Render.com**
   - Choose **Python** environment
   - Set **Start Command** as:
     ```bash
     gunicorn app:app
     ```

4. **Deploy on Heroku (Alternative)**

   - Install **Heroku CLI** and log in:
     ```bash
     heroku login
     ```
   - Create a **Procfile**:
     ```bash
     echo "web: gunicorn app:app" > Procfile
     ```
   - Push to **Heroku**:
     ```bash
     heroku create spam-detective-app
     git push heroku main
     heroku open
     ```

---

## **6. Summary**

- **Model**: **NaÃ¯ve Bayes Classifier with TF-IDF vectorization**
- **Web Framework**: **Flask** for API & UI
- **Storage**: **Pickle for model persistence**
- **Deployment Options**: **Render, Heroku, or PythonAnywhere**

With this setup, your **Spam Detective App is fully functional and deployed**. ðŸš€ If any enhancements are needed, feel free to modify the Flask routes, UI, or model parameters!

---

**All the best with your project!** ðŸŽ¯



